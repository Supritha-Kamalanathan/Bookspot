{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import psycopg2\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "db_host = os.getenv(\"DB_HOST\")\n",
    "db_user = os.getenv(\"DB_USER\")\n",
    "db_password = os.getenv(\"DB_PASSWORD\")\n",
    "db_port = os.getenv(\"DB_PORT\")\n",
    "db_name = os.getenv(\"DB_NAME\")\n",
    "\n",
    "# Connecting to database\n",
    "conn = psycopg2.connect(\n",
    "    dbname = db_name,\n",
    "    user = db_user,\n",
    "    password = db_password,\n",
    "    host = db_host,\n",
    "    port = db_port\n",
    ")\n",
    "\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the table if it doesn't already exist\n",
    "cursor.execute(\"CREATE TABLE IF NOT EXISTS books (book_id TEXT PRIMARY KEY, description TEXT)\")\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fetch data from the Open Library API and storing it in a database\n",
    "def fetch_books(subject, limit=1000):\n",
    "    url = f\"https://openlibrary.org/subjects/{subject}.json\"\n",
    "    params = {\n",
    "        'limit': 100,  \n",
    "        'offset': 0    \n",
    "    }\n",
    "\n",
    "    while limit > 0:\n",
    "        response = requests.get(url, params=params)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}\")\n",
    "            break\n",
    "        \n",
    "        data = response.json()\n",
    "        works = data.get('works', [])\n",
    "\n",
    "        for book in works:\n",
    "            id = book.get('availability', {}).get('identifier', '')\n",
    "            if not id:\n",
    "                continue\n",
    "            try:\n",
    "                description = f\"The title of the book is {book.get('title', '')} and was writen by {book.get('authors', {})[0].get('name', [])}. It was initially publish in the year {book.get('first_publish_year')}. Genres are {', '.join(book.get('subject', []))}.\"\n",
    "                cursor.execute(\"INSERT INTO books (book_id, description) VALUES (%s, %s) ON CONFLICT (book_id) DO NOTHING\", (id, description))\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        conn.commit()\n",
    "\n",
    "        params['offset'] += 100\n",
    "        limit -= 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching the data\n",
    "fetch_books('fiction', 5000)\n",
    "fetch_books('nonfiction', 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table for storing chunked data\n",
    "cursor.execute(\"CREATE TABLE IF NOT EXISTS chunks (chunk_id UUID PRIMARY KEY, b_id TEXT REFERENCES books(book_id), text TEXT, metadata JSONB, embeddings FLOAT8[])\")\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import uuid\n",
    "from transformers import AutoTokenizer\n",
    "import json as json\n",
    "\n",
    "# Function to tranform text data into manageable chunks for use in models\n",
    "def book_chunker(conn, \n",
    "                model_name, \n",
    "                chunk_size = 1024, \n",
    "                chunk_overlap = 0,  \n",
    "                separator = ' ', \n",
    "                secondary_chunking_regex = r'\\S+?[\\.,;!?]'):\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    books = {}\n",
    "\n",
    "    with conn.cursor() as cursor:\n",
    "        cursor.execute(\"SELECT * FROM books\")\n",
    "        rows = cursor.fetchall()\n",
    "\n",
    "    for id, desc in rows:\n",
    "        all_chunks = {}\n",
    "        words = desc.split(separator)\n",
    "        current_chunk = \"\"\n",
    "        chunks = []\n",
    "        for word in words:\n",
    "            new_chunk = current_chunk + (separator if current_chunk else '') + word\n",
    "            if len(tokenizer.tokenize(new_chunk)) <= chunk_size:\n",
    "                current_chunk = new_chunk\n",
    "            else:\n",
    "                if current_chunk:\n",
    "                    chunks.append(current_chunk)\n",
    "                current_chunk = word\n",
    "        \n",
    "        if current_chunk:\n",
    "            chunks.append(current_chunk)\n",
    "\n",
    "        refined_chunks = []\n",
    "        for chunk in chunks:\n",
    "            if len(tokenizer.tokenize(chunk)) > chunk_size:\n",
    "                sub_chunks = re.split(secondary_chunking_regex, chunk)\n",
    "                sub_chunk_accum = \"\"\n",
    "                for sub_chunk in sub_chunks:\n",
    "                    if sub_chunk_accum and len(tokenizer.tokenize(sub_chunk_accum + sub_chunk + ' ')) > chunk_size:\n",
    "                        refined_chunks.append(sub_chunk_accum.strip())\n",
    "                        sub_chunk_accum = sub_chunk\n",
    "                    else:\n",
    "                        sub_chunk_accum += (sub_chunk + ' ')\n",
    "                if sub_chunk_accum:\n",
    "                    refined_chunks.append(sub_chunk_accum.strip())\n",
    "            else:\n",
    "                refined_chunks.append(chunk)\n",
    "\n",
    "        final_chunks = []\n",
    "        if chunk_overlap > 0 and len(refined_chunks) > 1:\n",
    "            for i in range(len(refined_chunks) - 1):\n",
    "                final_chunks.append(refined_chunks[i])\n",
    "                overlap_start = max(0, len(refined_chunks[i]) - chunk_overlap)\n",
    "                overlap_end = min(chunk_overlap, len(refined_chunks[i + 1]))\n",
    "                overlap_chunk = refined_chunks[i][overlap_start:] + ' ' + refined_chunks[i + 1][:overlap_end]\n",
    "                final_chunks.append(overlap_chunk)\n",
    "            final_chunks.append(refined_chunks[-1])\n",
    "        else:\n",
    "            final_chunks = refined_chunks\n",
    "\n",
    "        for chunk in final_chunks:\n",
    "            chunk_id = str(uuid.uuid4())\n",
    "            all_chunks[chunk_id] = {\"text\": chunk, \"metadata\": {\"book_id\": id}}\n",
    "\n",
    "        books[id] = all_chunks\n",
    "\n",
    "    with conn.cursor() as cursor:\n",
    "        for book_id, chunks in books.items():\n",
    "            for chunk_id, chunk_data in chunks.items():\n",
    "                cursor.execute(\"INSERT INTO chunks (chunk_id, b_id, text, metadata) VALUES (%s, %s, %s, %s) ON CONFLICT (chunk_id) DO NOTHING\", (chunk_id, book_id, chunk_data[\"text\"], json.dumps(chunk_data[\"metadata\"])))\n",
    "        conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_chunker(conn, model_name='BAAI/bge-small-en-v1.5', chunk_size = 256, chunk_overlap = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the tokeniser and model locally\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "import os\n",
    "\n",
    "model_name = \"BAAI/bge-small-en-v1.5\"\n",
    "\n",
    "#  Loads the tokenizer associated with the specified model. The tokenizer is responsible for converting text into tokens that the model can understand.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Loads the pre-trained model itself, which will be used to generate embeddings from the tokenized text.\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "tokenizer_save_path = \"model/tokenizer\"\n",
    "model_save_path = \"model/embedding\"\n",
    "\n",
    "os.makedirs(tokenizer_save_path, exist_ok=True)\n",
    "os.makedirs(model_save_path, exist_ok=True)\n",
    "\n",
    "tokenizer.save_pretrained(tokenizer_save_path)\n",
    "model.save_pretrained(model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate embeddings for a given text using the loaded model and tokenizer\n",
    "def compute_embeddings(text):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_save_path)\n",
    "    model = AutoModel.from_pretrained(model_save_path)\n",
    "\n",
    "    inputs = tokenizer(text, return_tensors = \"pt\", padding = True, truncation = True)\n",
    "\n",
    "    # Temporarily disables gradient calculation which reduces memory usage and speeds up computation\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(**inputs).last_hidden_state.mean(dim = 1).squeeze()\n",
    "    \n",
    "    return embeddings.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that retrieves book descriptions and computes their embeddings\n",
    "def create_vector_store():\n",
    "    cursor.execute(\"SELECT chunk_id, text FROM chunks\")\n",
    "    rows = cursor.fetchall()\n",
    "\n",
    "    for chunk_id, text in rows:\n",
    "        embedding = compute_embeddings(text)\n",
    "        cursor.execute(\"UPDATE chunks SET embeddings = %s WHERE chunk_id = %s\", (embedding, chunk_id))\n",
    "        conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_vector_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function that computes embeddings of the query string, computes cosine similarity, and return top_k matches based on the scores\n",
    "def compute_matches(query_str, top_k):\n",
    "    query_str_embedding = np.array(compute_embeddings(query_str))\n",
    "    scores = []\n",
    "\n",
    "    cursor.execute(\"SELECT b_id, chunk_id, embeddings FROM chunks WHERE embeddings IS NOT NULL\")\n",
    "    rows = cursor.fetchall()\n",
    "\n",
    "    for book_id, chunk_id, chunk_embedding in rows:\n",
    "        chunk_embedding_array = np.array(chunk_embedding)\n",
    "\n",
    "        # Normalizing embeddings to unit vectors for cosine similarity calculation\n",
    "        norm_query = np.linalg.norm(query_str_embedding)\n",
    "        norm_chunk = np.linalg.norm(chunk_embedding_array)\n",
    "\n",
    "        if norm_query == 0 or norm_chunk == 0:\n",
    "            score = 0\n",
    "        else:\n",
    "            score = np.dot(chunk_embedding_array, query_str_embedding) / (norm_query * norm_chunk)\n",
    "\n",
    "        scores.append((book_id, chunk_id, score))\n",
    "\n",
    "    sorted_scores = sorted(scores, key = lambda item: item[2], reverse = True)[:top_k]\n",
    "    top_results = [(book_id, chunk_id, score) for (book_id, chunk_id, score) in sorted_scores]\n",
    "\n",
    "    return top_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Matches: \n",
      "Book ID: smertelnyiiadrom00saye, Chunk ID: 5c85bff1-c693-4b4d-9357-4c11b8b14db8, Similarity Score: 0.6613\n",
      "Matching text:  The title of the book is Strong Poison and was writen by Dorothy L. Sayers. It was initially publish in the year 1930. Genres are Women detectives, English Detective and mystery stories, Translations into Russian, Harriet Vane (Fictitious character), Private investigators, Lord Wimsey, Peter (Fictitious character), Fiction, Detective and mystery stories, Large type books, Apologetics, Dogma, Time, Fiction, mystery & detective, general, Wimsey, peter, lord (fictitious character), fiction, Fiction, mystery & detective, traditional, England, fiction.\n",
      "Book ID: lhotelbertram0000chri, Chunk ID: 35d7f6c5-8243-47cd-a55b-fdd79ef04afa, Similarity Score: 0.6429\n",
      "Matching text:  The title of the book is At Bertram's Hotel and was writen by Agatha Christie. It was initially publish in the year 1965. Genres are Fiction, Jane Marple (Fictitious character), Women detectives, Older women, History, Hotels, Mystery, Detective and mystery stories, Mystery fiction, Marple, Jane (Fictitious character), Fiction, mystery & detective, traditional, Marple, jane (fictitious character), fiction, Fiction, mystery & detective, women sleuths, England, fiction, English literature, Fiction, mystery & detective, general.\n",
      "Book ID: studyinscarletsi08doyl, Chunk ID: 1065c21e-70dc-445b-8249-a70811ccbf55, Similarity Score: 0.6380\n",
      "Matching text:  fiction, Mystery and detective stories.\n",
      "Book ID: hetzesdeslachtof0000patt_z6l9, Chunk ID: 18c9a484-3fd7-48fe-a0f4-e56722ee3d9e, Similarity Score: 0.6369\n",
      "Matching text:  The title of the book is The 6th Target and was writen by James Patterson. It was initially publish in the year 1728. Genres are Accessible book, Women detectives, Kidnapping, Women in the professions, Female friendship, Crimes against, Fiction, Children of the rich, Mentally ill offenders, Murderers, Protected DAISY, Policewomen, fiction, San francisco (calif.), fiction, Fiction, mystery & detective, women sleuths, Women's murder club (imaginary organization), fiction, Women's Murder Club (Imaginary organization), Lindsay Boxer (Fictitious character), Policewomen, Fiction, thrillers, suspense, Criminals, fiction, Friendship, fiction, Missing persons, fiction, Suspense, Thrillers, Fiction / Thrillers, Fiction - Espionage / Thriller, California, San Francisco, Large type books, collectionid:wmc.\n",
      "Book ID: synapseml_gutenberg_the_mysterious_affair_at_styles_by_agath, Chunk ID: 07f3c3e0-15d5-4a62-9318-eb64b16a18ea, Similarity Score: 0.6343\n",
      "Matching text:  The title of the book is The Mysterious Affair at Styles and was writen by Agatha Christie. It was initially publish in the year 1920. Genres are Fiction, Mystery, Crime Fiction, Mystery Thriller, Thriller, Adult, Classic Literature, British Literature, Hercule Poirot (Fictitious Character), Tommy Beresford (Fictitious Character), Tuppence Beresford (Fictitious Character), Captain Arthur Hastings (Fictitious Character), Private investigators, English Detectives, Murder, Investigation, Open Library Staff Picks, Books on CD, Private investigators, fiction, Fiction, mystery & detective, traditional, Poirot, hercule (fictitious character), fiction, England, fiction, Hastings, arthur, captain (fictitious character), fiction, Fiction, mystery & detective, general, Fiction, thrillers, general, Beresford, tommy (fictitious character), fiction, Beresford, tuppence (fictitious character), fiction, Married people, fiction, English literature, Language and languages, study and teaching, Crime, Fiction, general.\n"
     ]
    }
   ],
   "source": [
    "matches = compute_matches(\"Mystery novels with strong female leads\", 5)\n",
    "\n",
    "print(\"Top Matches: \")\n",
    "for book_id, chunk_id, score in matches:\n",
    "    print(f\"Book ID: {book_id}, Chunk ID: {chunk_id}, Similarity Score: {score:.4f}\")\n",
    "\n",
    "    cursor.execute(\"SELECT text FROM chunks WHERE chunk_id = %s\", (chunk_id,))\n",
    "    matching_text = cursor.fetchone()[0]\n",
    "    print(\"Matching text: \", matching_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "import sys\n",
    "\n",
    "def stream_and_buffer(prompt, llm, max_tokens = 3000, echo = True, stream = True):\n",
    "\n",
    "    formatted_prompt = f\"Q: {prompt} A: \"\n",
    "\n",
    "    res = llm(formatted_prompt, max_tokens = max_tokens, echo = echo, stream = stream)\n",
    "\n",
    "    buffer = \"\"\n",
    "\n",
    "    for message in res:\n",
    "        chunk = message['choices'][0]['text']\n",
    "        buffer += chunk\n",
    "\n",
    "        words = buffer.split(' ')\n",
    "        for word in words[:-1]:\n",
    "            sys.stdout.write(word + ' ')\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        buffer = words[-1]\n",
    "\n",
    "    if buffer:\n",
    "        sys.stdout.write(buffer)\n",
    "        sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to construct the prompt\n",
    "def construct_prompt(system_prompt, retrieved_data, user_query):\n",
    "    prompt = f\"\"\"{system_prompt}\n",
    "\n",
    "    Here is the user's query:\n",
    "    {user_query}\n",
    "\n",
    "    Here is the retrieved context:\n",
    "    {retrieved_data}\n",
    "\n",
    "    \"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to retrieve data of the top matches\n",
    "def retrieve_data(matches):\n",
    "    for match in matches:\n",
    "        book_id = match[0]\n",
    "        chunk_id = match[1]\n",
    "\n",
    "        cursor.execute(\"SELECT text FROM chunks WHERE b_id = %s AND chunk_id = %s\", (book_id, chunk_id))\n",
    "        data = cursor.fetchall()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from d:\\CDriveFolders\\Downloads\\mistral-7b-instruct-v0.2.Q3_K_L.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 13\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q3_K:  129 tensors\n",
      "llama_model_loader: - type q5_K:   96 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q3_K - Large\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 3.56 GiB (4.22 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.14 MiB\n",
      "llm_load_tensors:        CPU buffer size =  3644.27 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =    64.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    81.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'general.name': 'mistralai_mistral-7b-instruct-v0.2', 'general.architecture': 'llama', 'llama.context_length': '32768', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '13', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '1000000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\"}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Guessed chat format: mistral-instruct\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "     Based on the information provided, I cannot directly recommend a must-read non-fiction book from the last decade since the context only mentions the book \"How Not to Act Old\" by Pamela Redmond Satran, which is a humorous take on aging. To provide a more accurate recommendation, I would need more information about the user's interests, preferences, or specific topics they are interested in. However, some highly acclaimed non-fiction books from the last decade include:\n",
      "\n",
      "1. \"Sapiens: A Brief History of Humankind\" by Yuval Noah Harari (2014) - An engaging and thought-provoking exploration of human history, evolution, and the future.\n",
      "\n",
      "2. \"Thinking, Fast and Slow\" by Daniel Kahneman (2011) - A Pulitzer Prize-winning exploration of the human mind and the way we make decisions.\n",
      "\n",
      "3. \"Between the World and Me\" by Ta-Nehisi Coates (2015) - An eloquent and powerful exploration of race and identity in America.\n",
      "\n",
      "4. \"Moonwalking with Einstein: The Art and Science of Learning Memory\" by Joshua Foer "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   46686.98 ms\n",
      "llama_print_timings:      sample time =      29.84 ms /   264 runs   (    0.11 ms per token,  8848.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =   46686.69 ms /   248 tokens (  188.25 ms per token,     5.31 tokens per second)\n",
      "llama_print_timings:        eval time =   95796.43 ms /   263 runs   (  364.24 ms per token,     2.75 tokens per second)\n",
      "llama_print_timings:       total time =  143190.91 ms /   511 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(201"
     ]
    }
   ],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a knowledgeable and helpful book recommendation system. Your task is to provide book recommendations and insights based on the context provided. You should rely solely on the information given in the context to generate your responses. Do not include any information that is not present in the context and don't mention 'context' in your answer. Focus on providing relevant and accurate recommendations or answers according to the book descriptions and details provided.\n",
    "\"\"\"\n",
    "\n",
    "user_query = \"Suggest some must-read non-fiction book from the last decade\"\n",
    "\n",
    "matches = compute_matches(user_query, 5)\n",
    "retrieved_data = retrieve_data(matches)\n",
    "prompt = construct_prompt(system_prompt, retrieved_data, user_query)\n",
    "\n",
    "model_path = \"model/mistral-7b-instruct-v0.2.Q3_K_L.gguf\"\n",
    "\n",
    "llm = Llama(model_path=model_path, n_gpu_layers=1)\n",
    "\n",
    "stream_and_buffer(prompt, llm, echo = False, max_tokens=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
